// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0
// source: entry.sql

package queries

import (
	"context"

	"github.com/jackc/pgx/v5/pgtype"
	"github.com/pgvector/pgvector-go"
	"go.trulyao.dev/hubble/web/pkg/document"
	"go.trulyao.dev/hubble/web/pkg/rbac"
)

const chunkCanBeProcessed = `-- name: ChunkCanBeProcessed :one
select count(*) > 0 as can_process
from entry_chunks ec
where
    ec.id = $1
    and (
        ec.embedding_status = 'pending'
        or (ec.embedding_status = 'failed' and ec.embedding_error_count < 5)
        or (
            ec.embedding_status = 'processing'
            and ec.embedding_status_updated_at < now() - interval '2 hours'
        )
    )
    and ec.semantic_vector is null
    and ec.content is not null
    and ec.content != ''
`

// the actual data is passed to the handlers in the queue, but to prevent duplicate
// jobs, handlers need to ensure they can actually process what they have just gotten
func (q *Queries) ChunkCanBeProcessed(ctx context.Context, chunkID int32) (bool, error) {
	row := q.db.QueryRow(ctx, chunkCanBeProcessed, chunkID)
	var can_process bool
	err := row.Scan(&can_process)
	return can_process, err
}

const createFileEntry = `-- name: CreateFileEntry :one
insert into entries (name, meta, content, file_id, entry_type, checksum, collection_id, added_by, last_updated_by, filesize_bytes) values ($1, $2, $3, $4, $5, $6, $7, $8, $8, $9) returning id, origin, name, content, file_id, version, entry_type, checksum, parent_id, collection_id, added_by, last_updated_by, meta, created_at, updated_at, deleted_at, archived_at, filesize_bytes, public_id, text_content
`

type CreateFileEntryParams struct {
	Name          string             `json:"name"`
	Meta          []byte             `json:"meta"`
	Content       pgtype.Text        `json:"content"`
	FileID        pgtype.Text        `json:"file_id"`
	EntryType     document.EntryType `json:"entry_type"`
	Checksum      pgtype.Text        `json:"checksum"`
	CollectionID  int32              `json:"collection_id"`
	AddedBy       int32              `json:"added_by"`
	FilesizeBytes int64              `json:"filesize_bytes"`
}

func (q *Queries) CreateFileEntry(ctx context.Context, arg CreateFileEntryParams) (Entry, error) {
	row := q.db.QueryRow(ctx, createFileEntry,
		arg.Name,
		arg.Meta,
		arg.Content,
		arg.FileID,
		arg.EntryType,
		arg.Checksum,
		arg.CollectionID,
		arg.AddedBy,
		arg.FilesizeBytes,
	)
	var i Entry
	err := row.Scan(
		&i.ID,
		&i.Origin,
		&i.Name,
		&i.Content,
		&i.FileID,
		&i.Version,
		&i.EntryType,
		&i.Checksum,
		&i.ParentID,
		&i.CollectionID,
		&i.AddedBy,
		&i.LastUpdatedBy,
		&i.Meta,
		&i.CreatedAt,
		&i.UpdatedAt,
		&i.DeletedAt,
		&i.ArchivedAt,
		&i.FilesizeBytes,
		&i.PublicID,
		&i.TextContent,
	)
	return i, err
}

const createLinkEntry = `-- name: CreateLinkEntry :one
insert into entries (name, meta, version, entry_type, collection_id, added_by, last_updated_by) values ($1, $2, 1, 'link', $3, $4, $4) returning id, origin, name, content, file_id, version, entry_type, checksum, parent_id, collection_id, added_by, last_updated_by, meta, created_at, updated_at, deleted_at, archived_at, filesize_bytes, public_id, text_content
`

type CreateLinkEntryParams struct {
	Name         string `json:"name"`
	Meta         []byte `json:"meta"`
	CollectionID int32  `json:"collection_id"`
	UserID       int32  `json:"user_id"`
}

func (q *Queries) CreateLinkEntry(ctx context.Context, arg CreateLinkEntryParams) (Entry, error) {
	row := q.db.QueryRow(ctx, createLinkEntry,
		arg.Name,
		arg.Meta,
		arg.CollectionID,
		arg.UserID,
	)
	var i Entry
	err := row.Scan(
		&i.ID,
		&i.Origin,
		&i.Name,
		&i.Content,
		&i.FileID,
		&i.Version,
		&i.EntryType,
		&i.Checksum,
		&i.ParentID,
		&i.CollectionID,
		&i.AddedBy,
		&i.LastUpdatedBy,
		&i.Meta,
		&i.CreatedAt,
		&i.UpdatedAt,
		&i.DeletedAt,
		&i.ArchivedAt,
		&i.FilesizeBytes,
		&i.PublicID,
		&i.TextContent,
	)
	return i, err
}

const deleteEntries = `-- name: DeleteEntries :many
delete from entries
where public_id = any($1::uuid[]) and deleted_at is null
returning public_id
`

func (q *Queries) DeleteEntries(ctx context.Context, entryPublicIds []pgtype.UUID) ([]pgtype.UUID, error) {
	rows, err := q.db.Query(ctx, deleteEntries, entryPublicIds)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []pgtype.UUID{}
	for rows.Next() {
		var public_id pgtype.UUID
		if err := rows.Scan(&public_id); err != nil {
			return nil, err
		}
		items = append(items, public_id)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const deleteEntryChunksByPublicId = `-- name: DeleteEntryChunksByPublicId :many
with
    targets as (
        select e.id, e.version
        from entries e
        join collections c on c.id = e.collection_id
        join workspaces w on w.id = c.workspace_id
        where
            e.public_id = any($1::uuid[])
            and w.public_id = $2
            and e.deleted_at is null
    )
delete from entry_chunks ec
using targets t
where ec.entry_id = t.id and ec.min_version >= t.version
returning ec.entry_id::integer
`

type DeleteEntryChunksByPublicIdParams struct {
	EntryPublicIds []pgtype.UUID `json:"entry_public_ids"`
	WorkspaceID    pgtype.UUID   `json:"workspace_id"`
}

func (q *Queries) DeleteEntryChunksByPublicId(ctx context.Context, arg DeleteEntryChunksByPublicIdParams) ([]int32, error) {
	rows, err := q.db.Query(ctx, deleteEntryChunksByPublicId, arg.EntryPublicIds, arg.WorkspaceID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []int32{}
	for rows.Next() {
		var ec_entry_id int32
		if err := rows.Scan(&ec_entry_id); err != nil {
			return nil, err
		}
		items = append(items, ec_entry_id)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const dequeueEntries = `-- name: DequeueEntries :many
delete from entries_queue eq
using entries e
where eq.entry_id = e.id and e.public_id = any($1::uuid[])
returning eq.entry_id
`

func (q *Queries) DequeueEntries(ctx context.Context, entryPublicIds []pgtype.UUID) ([]int32, error) {
	rows, err := q.db.Query(ctx, dequeueEntries, entryPublicIds)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []int32{}
	for rows.Next() {
		var entry_id int32
		if err := rows.Scan(&entry_id); err != nil {
			return nil, err
		}
		items = append(items, entry_id)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

type EnqueueEntriesParams struct {
	EntryID int32                 `json:"entry_id"`
	Payload document.QueuePayload `json:"payload"`
}

const findAllQueuedEntries = `-- name: FindAllQueuedEntries :many
select e.id
from entries_queue q
join entries e on e.id = q.entry_id
where
    q.attempts <= q.max_attempts
    and (
        q.status in ('queued', 'failed')  -- either it hasn't been processed or it failed
        or (q.status = 'processing' and q.updated_at < now() - interval '12 hours')  -- or it has been marked as processing but hasn't been updated in the last 12 hours
    )
    and q.available_at <= now()
    and e.deleted_at is null
`

func (q *Queries) FindAllQueuedEntries(ctx context.Context) ([]int32, error) {
	rows, err := q.db.Query(ctx, findAllQueuedEntries)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []int32{}
	for rows.Next() {
		var id int32
		if err := rows.Scan(&id); err != nil {
			return nil, err
		}
		items = append(items, id)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const findEntries = `-- name: FindEntries :many
with
    latest_entries as (
        select
            e.id, e.origin, e.name, e.content, e.file_id, e.version, e.entry_type, e.checksum, e.parent_id, e.collection_id, e.added_by, e.last_updated_by, e.meta, e.created_at, e.updated_at, e.deleted_at, e.archived_at, e.filesize_bytes, e.public_id, e.text_content,
            row_number() over (
                partition by coalesce(e.parent_id, e.id) order by e.version desc
            ) as rn
        from entries e
        join collections c on c.id = e.collection_id
        join workspaces w on w.id = c.workspace_id
        join collection_members cm on cm.collection_id = c.id
        join workspace_members wm on wm.workspace_id = w.id
        where
            cm.user_id = $3
            and wm.user_id = $3
            and e.deleted_at is null
            and e.archived_at is null
            and (
                $4::text is null
                or w.slug = $4::text
            )
            and (
                $5::uuid is null
                or w.public_id = $5::uuid
            )
            and (
                $6::text is null
                or c.slug = $6::text
            )
            and (
                $7::uuid is null
                or c.public_id = $7::uuid
            )
            and c.deleted_at is null
            and w.deleted_at is null
            and e.deleted_at is null
            and e.archived_at is null
        order by e.collection_id, e.version desc
    )
select
    e.id,
    e.public_id,
    e.parent_id,
    e.origin,
    e.content,
    e.text_content,
    e.name,
    e.meta,
    e.version,
    e.entry_type as type,
    e.file_id,
    e.filesize_bytes,
    e.added_by,
    e.last_updated_by,
    e.created_at,
    e.updated_at,
    e.archived_at,
    c.name as collection_name,
    c.public_id as collection_id,
    c.slug as collection_slug,
    u.first_name as added_by_first_name,
    u.last_name as added_by_last_name,
    u.username as added_by_username,
    u.avatar_id as added_by_avatar_id,
    w.public_id as workspace_id,
    w.display_name as workspace_name,
    w.slug as workspace_slug,
    q.status,
    q.created_at as queued_at,
    count(*) over () as total_entries
from latest_entries e
join collections c on c.id = e.collection_id
join workspaces w on w.id = c.workspace_id
join entries_queue q on q.entry_id = e.id
join users u on u.id = e.added_by
order by coalesce(e.updated_at, e.created_at) desc, q.updated_at desc
limit $1
offset $2
`

type FindEntriesParams struct {
	Limit              int32       `json:"limit"`
	Offset             int32       `json:"offset"`
	UserID             int32       `json:"user_id"`
	WorkspaceSlug      pgtype.Text `json:"workspace_slug"`
	WorkspacePublicID  pgtype.UUID `json:"workspace_public_id"`
	CollectionSlug     pgtype.Text `json:"collection_slug"`
	CollectionPublicID pgtype.UUID `json:"collection_public_id"`
}

type FindEntriesRow struct {
	ID               int32              `json:"id"`
	PublicID         pgtype.UUID        `json:"public_id"`
	ParentID         pgtype.Int4        `json:"parent_id"`
	Origin           pgtype.UUID        `json:"origin"`
	Content          pgtype.Text        `json:"content"`
	TextContent      pgtype.Text        `json:"text_content"`
	Name             string             `json:"name"`
	Meta             []byte             `json:"meta"`
	Version          int32              `json:"version"`
	Type             string             `json:"type"`
	FileID           pgtype.Text        `json:"file_id"`
	FilesizeBytes    int64              `json:"filesize_bytes"`
	AddedBy          int32              `json:"added_by"`
	LastUpdatedBy    int32              `json:"last_updated_by"`
	CreatedAt        pgtype.Timestamptz `json:"created_at"`
	UpdatedAt        pgtype.Timestamptz `json:"updated_at"`
	ArchivedAt       pgtype.Timestamptz `json:"archived_at"`
	CollectionName   string             `json:"collection_name"`
	CollectionID     pgtype.UUID        `json:"collection_id"`
	CollectionSlug   pgtype.Text        `json:"collection_slug"`
	AddedByFirstName string             `json:"added_by_first_name"`
	AddedByLastName  string             `json:"added_by_last_name"`
	AddedByUsername  string             `json:"added_by_username"`
	AddedByAvatarID  pgtype.Text        `json:"added_by_avatar_id"`
	WorkspaceID      pgtype.UUID        `json:"workspace_id"`
	WorkspaceName    string             `json:"workspace_name"`
	WorkspaceSlug    pgtype.Text        `json:"workspace_slug"`
	Status           EntryStatus        `json:"status"`
	QueuedAt         pgtype.Timestamp   `json:"queued_at"`
	TotalEntries     int64              `json:"total_entries"`
}

func (q *Queries) FindEntries(ctx context.Context, arg FindEntriesParams) ([]FindEntriesRow, error) {
	rows, err := q.db.Query(ctx, findEntries,
		arg.Limit,
		arg.Offset,
		arg.UserID,
		arg.WorkspaceSlug,
		arg.WorkspacePublicID,
		arg.CollectionSlug,
		arg.CollectionPublicID,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []FindEntriesRow{}
	for rows.Next() {
		var i FindEntriesRow
		if err := rows.Scan(
			&i.ID,
			&i.PublicID,
			&i.ParentID,
			&i.Origin,
			&i.Content,
			&i.TextContent,
			&i.Name,
			&i.Meta,
			&i.Version,
			&i.Type,
			&i.FileID,
			&i.FilesizeBytes,
			&i.AddedBy,
			&i.LastUpdatedBy,
			&i.CreatedAt,
			&i.UpdatedAt,
			&i.ArchivedAt,
			&i.CollectionName,
			&i.CollectionID,
			&i.CollectionSlug,
			&i.AddedByFirstName,
			&i.AddedByLastName,
			&i.AddedByUsername,
			&i.AddedByAvatarID,
			&i.WorkspaceID,
			&i.WorkspaceName,
			&i.WorkspaceSlug,
			&i.Status,
			&i.QueuedAt,
			&i.TotalEntries,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const findEntryById = `-- name: FindEntryById :one
select
    e.id,
    e.public_id,
    e.parent_id,
    e.origin,
    e.content,
    e.text_content,
    e.name,
    e.meta,
    e.version,
    e.entry_type as type,
    e.file_id,
    e.filesize_bytes,
    e.added_by,
    e.last_updated_by,
    e.created_at,
    e.updated_at,
    e.archived_at,
    c.name as collection_name,
    c.public_id as collection_id,
    c.slug as collection_slug,
    u.first_name as added_by_first_name,
    u.last_name as added_by_last_name,
    u.username as added_by_username,
    u.avatar_id as added_by_avatar_id,
    w.public_id as workspace_id,
    w.display_name as workspace_name,
    w.slug as workspace_slug,
    q.status,
    q.created_at as queued_at
from entries e
join collections c on c.id = e.collection_id
join workspaces w on w.id = c.workspace_id
join entries_queue q on q.entry_id = e.id
join users u on u.id = e.added_by
where
    (
        ($1::uuid is null and e.id = $2)
        or ($2::integer is null and e.public_id = $1)
    )
    and ($3::text is null or w.slug = $3)
    and (
        $4::uuid is null
        or w.public_id = $4
    )
    and ($5::text is null or c.slug = $5)
    and (
        $6::uuid is null
        or c.public_id = $6
    )
    and e.deleted_at is null
    and c.deleted_at is null
    and w.deleted_at is null
limit 1
`

type FindEntryByIdParams struct {
	EntryPublicID      pgtype.UUID `json:"entry_public_id"`
	EntryID            pgtype.Int4 `json:"entry_id"`
	WorkspaceSlug      pgtype.Text `json:"workspace_slug"`
	WorkspacePublicID  pgtype.UUID `json:"workspace_public_id"`
	CollectionSlug     pgtype.Text `json:"collection_slug"`
	CollectionPublicID pgtype.UUID `json:"collection_public_id"`
}

type FindEntryByIdRow struct {
	ID               int32              `json:"id"`
	PublicID         pgtype.UUID        `json:"public_id"`
	ParentID         pgtype.Int4        `json:"parent_id"`
	Origin           pgtype.UUID        `json:"origin"`
	Content          pgtype.Text        `json:"content"`
	TextContent      pgtype.Text        `json:"text_content"`
	Name             string             `json:"name"`
	Meta             []byte             `json:"meta"`
	Version          int32              `json:"version"`
	Type             document.EntryType `json:"type"`
	FileID           pgtype.Text        `json:"file_id"`
	FilesizeBytes    int64              `json:"filesize_bytes"`
	AddedBy          int32              `json:"added_by"`
	LastUpdatedBy    int32              `json:"last_updated_by"`
	CreatedAt        pgtype.Timestamptz `json:"created_at"`
	UpdatedAt        pgtype.Timestamptz `json:"updated_at"`
	ArchivedAt       pgtype.Timestamptz `json:"archived_at"`
	CollectionName   string             `json:"collection_name"`
	CollectionID     pgtype.UUID        `json:"collection_id"`
	CollectionSlug   pgtype.Text        `json:"collection_slug"`
	AddedByFirstName string             `json:"added_by_first_name"`
	AddedByLastName  string             `json:"added_by_last_name"`
	AddedByUsername  string             `json:"added_by_username"`
	AddedByAvatarID  pgtype.Text        `json:"added_by_avatar_id"`
	WorkspaceID      pgtype.UUID        `json:"workspace_id"`
	WorkspaceName    string             `json:"workspace_name"`
	WorkspaceSlug    pgtype.Text        `json:"workspace_slug"`
	Status           EntryStatus        `json:"status"`
	QueuedAt         pgtype.Timestamp   `json:"queued_at"`
}

func (q *Queries) FindEntryById(ctx context.Context, arg FindEntryByIdParams) (FindEntryByIdRow, error) {
	row := q.db.QueryRow(ctx, findEntryById,
		arg.EntryPublicID,
		arg.EntryID,
		arg.WorkspaceSlug,
		arg.WorkspacePublicID,
		arg.CollectionSlug,
		arg.CollectionPublicID,
	)
	var i FindEntryByIdRow
	err := row.Scan(
		&i.ID,
		&i.PublicID,
		&i.ParentID,
		&i.Origin,
		&i.Content,
		&i.TextContent,
		&i.Name,
		&i.Meta,
		&i.Version,
		&i.Type,
		&i.FileID,
		&i.FilesizeBytes,
		&i.AddedBy,
		&i.LastUpdatedBy,
		&i.CreatedAt,
		&i.UpdatedAt,
		&i.ArchivedAt,
		&i.CollectionName,
		&i.CollectionID,
		&i.CollectionSlug,
		&i.AddedByFirstName,
		&i.AddedByLastName,
		&i.AddedByUsername,
		&i.AddedByAvatarID,
		&i.WorkspaceID,
		&i.WorkspaceName,
		&i.WorkspaceSlug,
		&i.Status,
		&i.QueuedAt,
	)
	return i, err
}

const findEntryChunks = `-- name: FindEntryChunks :many
select ec.id, ec.content, ec.embedding_status
from entry_chunks ec
join entries e on e.id = ec.entry_id
where
    e.public_id = $1
    and (
        ec.embedding_status = 'pending'
        -- we will only retry failed chunks that have failed less than 5 times
        or (ec.embedding_status = 'failed' and ec.embedding_error_count < 5)
        or (
            -- this is a special case where the chunk was marked as processing but
            -- hasn't
            -- been updated in the last 2 hours
            ec.embedding_status = 'processing'
            and ec.embedding_status_updated_at < now() - interval '2 hours'
        )
    )
    and ec.semantic_vector is null
    and ec.content is not null
    and ec.content != ''
    and e.deleted_at is null
`

type FindEntryChunksRow struct {
	ID              int32                     `json:"id"`
	Content         pgtype.Text               `json:"content"`
	EmbeddingStatus EntryChunkEmbeddingStatus `json:"embedding_status"`
}

func (q *Queries) FindEntryChunks(ctx context.Context, entryPublicID pgtype.UUID) ([]FindEntryChunksRow, error) {
	rows, err := q.db.Query(ctx, findEntryChunks, entryPublicID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []FindEntryChunksRow{}
	for rows.Next() {
		var i FindEntryChunksRow
		if err := rows.Scan(&i.ID, &i.Content, &i.EmbeddingStatus); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const findEntryInCollectionAndWorkspace = `-- name: FindEntryInCollectionAndWorkspace :one
select
    e.id, e.origin, e.name, e.content, e.file_id, e.version, e.entry_type, e.checksum, e.parent_id, e.collection_id, e.added_by, e.last_updated_by, e.meta, e.created_at, e.updated_at, e.deleted_at, e.archived_at, e.filesize_bytes, e.public_id, e.text_content,
    c.id, c.public_id, c.name, c.workspace_id, c.description, c.avatar_id, c.created_at, c.updated_at, c.deleted_at, c.slug, c.owner_id,
    u.first_name as added_by_first_name,
    u.last_name as added_by_last_name,
    u.username as added_by_username,
    q.status,
    q.created_at as queued_at
from entries e
join collections c on c.id = e.collection_id
join workspaces w on w.id = c.workspace_id
join entries_queue q on q.entry_id = e.id
join users u on u.id = e.added_by
where
    e.public_id = $1
    and w.public_id = $2
    and c.public_id = $3
    and e.deleted_at is null
    and c.deleted_at is null
    and w.deleted_at is null
`

type FindEntryInCollectionAndWorkspaceParams struct {
	EntryPublicID pgtype.UUID `json:"entry_public_id"`
	WorkspaceID   pgtype.UUID `json:"workspace_id"`
	CollectionID  pgtype.UUID `json:"collection_id"`
}

type FindEntryInCollectionAndWorkspaceRow struct {
	Entry            Entry            `json:"entry"`
	Collection       Collection       `json:"collection"`
	AddedByFirstName string           `json:"added_by_first_name"`
	AddedByLastName  string           `json:"added_by_last_name"`
	AddedByUsername  string           `json:"added_by_username"`
	Status           EntryStatus      `json:"status"`
	QueuedAt         pgtype.Timestamp `json:"queued_at"`
}

func (q *Queries) FindEntryInCollectionAndWorkspace(ctx context.Context, arg FindEntryInCollectionAndWorkspaceParams) (FindEntryInCollectionAndWorkspaceRow, error) {
	row := q.db.QueryRow(ctx, findEntryInCollectionAndWorkspace, arg.EntryPublicID, arg.WorkspaceID, arg.CollectionID)
	var i FindEntryInCollectionAndWorkspaceRow
	err := row.Scan(
		&i.Entry.ID,
		&i.Entry.Origin,
		&i.Entry.Name,
		&i.Entry.Content,
		&i.Entry.FileID,
		&i.Entry.Version,
		&i.Entry.EntryType,
		&i.Entry.Checksum,
		&i.Entry.ParentID,
		&i.Entry.CollectionID,
		&i.Entry.AddedBy,
		&i.Entry.LastUpdatedBy,
		&i.Entry.Meta,
		&i.Entry.CreatedAt,
		&i.Entry.UpdatedAt,
		&i.Entry.DeletedAt,
		&i.Entry.ArchivedAt,
		&i.Entry.FilesizeBytes,
		&i.Entry.PublicID,
		&i.Entry.TextContent,
		&i.Collection.ID,
		&i.Collection.PublicID,
		&i.Collection.Name,
		&i.Collection.WorkspaceID,
		&i.Collection.Description,
		&i.Collection.AvatarID,
		&i.Collection.CreatedAt,
		&i.Collection.UpdatedAt,
		&i.Collection.DeletedAt,
		&i.Collection.Slug,
		&i.Collection.OwnerID,
		&i.AddedByFirstName,
		&i.AddedByLastName,
		&i.AddedByUsername,
		&i.Status,
		&i.QueuedAt,
	)
	return i, err
}

const findUnindexedChunks = `-- name: FindUnindexedChunks :many
select id, content
from entry_chunks
where
    embedding_status = 'pending'
    -- we will only retry failed chunks that have failed less than 5 times
    or (embedding_status = 'failed' and embedding_error_count < 5)
    or (
        -- this is a special case where the chunk was marked as processing but hasn't
        -- been updated in the last 2 hours
        embedding_status = 'processing'
        and embedding_status_updated_at < now() - interval '2 hours'
    )
    and semantic_vector is null
    and content is not null
    and content != ''
`

type FindUnindexedChunksRow struct {
	ID      int32       `json:"id"`
	Content pgtype.Text `json:"content"`
}

func (q *Queries) FindUnindexedChunks(ctx context.Context) ([]FindUnindexedChunksRow, error) {
	rows, err := q.db.Query(ctx, findUnindexedChunks)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []FindUnindexedChunksRow{}
	for rows.Next() {
		var i FindUnindexedChunksRow
		if err := rows.Scan(&i.ID, &i.Content); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getEntriesOwnership = `-- name: GetEntriesOwnership :many
select
    e.public_id,
    e.added_by,
    (e.added_by = cm.user_id) as is_owner,
    cm.bitmask_role as user_role
from entries e
join collection_members cm on cm.collection_id = e.collection_id
where cm.user_id = $1 and e.public_id = any($2::uuid[])
`

type GetEntriesOwnershipParams struct {
	UserID         int32         `json:"user_id"`
	EntryPublicIds []pgtype.UUID `json:"entry_public_ids"`
}

type GetEntriesOwnershipRow struct {
	PublicID pgtype.UUID `json:"public_id"`
	AddedBy  int32       `json:"added_by"`
	IsOwner  bool        `json:"is_owner"`
	UserRole rbac.Role   `json:"user_role"`
}

func (q *Queries) GetEntriesOwnership(ctx context.Context, arg GetEntriesOwnershipParams) ([]GetEntriesOwnershipRow, error) {
	rows, err := q.db.Query(ctx, getEntriesOwnership, arg.UserID, arg.EntryPublicIds)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []GetEntriesOwnershipRow{}
	for rows.Next() {
		var i GetEntriesOwnershipRow
		if err := rows.Scan(
			&i.PublicID,
			&i.AddedBy,
			&i.IsOwner,
			&i.UserRole,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const insertChunk = `-- name: InsertChunk :one
with
    entry_id as (
        select id from entries where public_id = $5 and deleted_at is null
    )
    insert into entry_chunks(entry_id, chunk_index, min_version, content, language)
select id, $1, $2, $3, $4
from entry_id
returning id, entry_id, chunk_index, min_version, content, created_at, updated_at, deleted_at, text_vector, semantic_vector, language, embedding_status, embedding_status_updated_at, last_embedding_error, last_embedding_error_at, embedding_error_count
`

type InsertChunkParams struct {
	Index         int32       `json:"index"`
	MinVersion    int32       `json:"min_version"`
	Content       pgtype.Text `json:"content"`
	Language      pgtype.Text `json:"language"`
	EntryPublicID pgtype.UUID `json:"entry_public_id"`
}

func (q *Queries) InsertChunk(ctx context.Context, arg InsertChunkParams) (EntryChunk, error) {
	row := q.db.QueryRow(ctx, insertChunk,
		arg.Index,
		arg.MinVersion,
		arg.Content,
		arg.Language,
		arg.EntryPublicID,
	)
	var i EntryChunk
	err := row.Scan(
		&i.ID,
		&i.EntryID,
		&i.ChunkIndex,
		&i.MinVersion,
		&i.Content,
		&i.CreatedAt,
		&i.UpdatedAt,
		&i.DeletedAt,
		&i.TextVector,
		&i.SemanticVector,
		&i.Language,
		&i.EmbeddingStatus,
		&i.EmbeddingStatusUpdatedAt,
		&i.LastEmbeddingError,
		&i.LastEmbeddingErrorAt,
		&i.EmbeddingErrorCount,
	)
	return i, err
}

type InsertChunksParams struct {
	EntryID    pgtype.Int4 `json:"entry_id"`
	Index      int32       `json:"index"`
	MinVersion int32       `json:"min_version"`
	Content    pgtype.Text `json:"content"`
	Language   pgtype.Text `json:"language"`
}

const queryWithHybridSearch = `-- name: QueryWithHybridSearch :many
with
    semantic_search as (
        select
            e.public_id,
            e.name as title,
            e.meta,
            e.entry_type as type,
            e.file_id,
            e.filesize_bytes,
            e.created_at,
            e.updated_at,
            e.archived_at,
            c.name as collection_name,
            c.public_id as collection_id,
            c.slug as collection_slug,
            w.display_name as workspace_name,
            w.public_id as workspace_id,
            w.slug as workspace_slug,
            ck.id as chunk_id,
            ck.content as chunk_content,
            ck.chunk_index,
            q.status,
            null::float8 as text_score,
            ($3 <=> ck.semantic_vector)::float8 as semantic_score,
            rank() over (order by $3 <=> ck.semantic_vector) as rank
        from entry_chunks ck
        join entries e on e.id = ck.entry_id
        join collections c on c.id = e.collection_id
        join workspaces w on w.id = c.workspace_id
        join entries_queue q on q.entry_id = e.id
        join workspace_members wm on wm.workspace_id = w.id
        join collection_members cm on cm.collection_id = c.id
        where
            q.status = 'completed'
            and e.archived_at is null
            and ck.semantic_vector is not null
            and (
                $4::uuid is null
                or w.public_id = $4
            )
            and ($5::text is null or w.slug = $5)
            and wm.user_id = $6
            and cm.user_id = $6
            and e.deleted_at is null
            and c.deleted_at is null
            and w.deleted_at is null
        order by $3 <=> ck.semantic_vector
        limit $1
        offset $2
    ),
    text_search as (
        select
            e.public_id,
            e.name as title,
            e.meta,
            e.entry_type as type,
            e.file_id,
            e.filesize_bytes,
            e.created_at,
            e.updated_at,
            e.archived_at,
            c.name as collection_name,
            c.public_id as collection_id,
            c.slug as collection_slug,
            w.display_name as workspace_name,
            w.public_id as workspace_id,
            w.slug as workspace_slug,
            ck.id as chunk_id,
            ck.content as chunk_content,
            ck.chunk_index,
            q.status,
            ts_rank(
                ck.text_vector, websearch_to_tsquery(ts_regconfig(ck.language), $7)
            ) as text_score,
            0.0::float8 as semantic_score,
            rank() over (
                order by
                    ts_rank(
                        ck.text_vector,
                        websearch_to_tsquery(ts_regconfig(ck.language), $7)
                    )
            ) as rank
        from entry_chunks ck
        join entries e on e.id = ck.entry_id
        join collections c on c.id = e.collection_id
        join workspaces w on w.id = c.workspace_id
        join entries_queue q on q.entry_id = e.id
        join workspace_members wm on wm.workspace_id = w.id
        join collection_members cm on cm.collection_id = c.id
        where
            (ck.text_vector @@ websearch_to_tsquery(ts_regconfig(ck.language), $7))
            and q.status = 'completed'
            and e.archived_at is null
            and ck.text_vector is not null
            and (
                $4::uuid is null
                or w.public_id = $4
            )
            and ($5::text is null or w.slug = $5)
            and wm.user_id = $6
            and cm.user_id = $6
            and e.deleted_at is null
            and c.deleted_at is null
            and w.deleted_at is null
        order by rank
        limit $1
        offset $2
    )
select public_id, title, meta, type, file_id, filesize_bytes, created_at, updated_at, archived_at, collection_name, collection_id, collection_slug, workspace_name, workspace_id, workspace_slug, chunk_id, chunk_content, chunk_index, status, text_score, semantic_score, rank, sum(coalesce(1.0 / (results.rank + 50), 0.0))::float8 as score
from
    (
        select public_id, title, meta, type, file_id, filesize_bytes, created_at, updated_at, archived_at, collection_name, collection_id, collection_slug, workspace_name, workspace_id, workspace_slug, chunk_id, chunk_content, chunk_index, status, text_score, semantic_score, rank
        from semantic_search
        union all
        select public_id, title, meta, type, file_id, filesize_bytes, created_at, updated_at, archived_at, collection_name, collection_id, collection_slug, workspace_name, workspace_id, workspace_slug, chunk_id, chunk_content, chunk_index, status, text_score, semantic_score, rank
        from text_search
    ) as results
group by
    results.public_id,
    results.title,
    results.meta,
    results.type,
    results.file_id,
    results.filesize_bytes,
    results.created_at,
    results.updated_at,
    results.archived_at,
    results.collection_name,
    results.collection_id,
    results.collection_slug,
    results.workspace_name,
    results.workspace_id,
    results.workspace_slug,
    results.chunk_id,
    results.chunk_content,
    results.chunk_index,
    results.text_score,
    results.semantic_score,
    results.rank,
    results.status
order by score desc
limit $1
offset $2
`

type QueryWithHybridSearchParams struct {
	Limit             int32           `json:"limit"`
	Offset            int32           `json:"offset"`
	Embedding         pgvector.Vector `json:"embedding"`
	WorkspacePublicID pgtype.UUID     `json:"workspace_public_id"`
	WorkspaceSlug     pgtype.Text     `json:"workspace_slug"`
	UserID            int32           `json:"user_id"`
	Query             string          `json:"query"`
}

type QueryWithHybridSearchRow struct {
	PublicID       pgtype.UUID        `json:"public_id"`
	Title          string             `json:"title"`
	Meta           []byte             `json:"meta"`
	Type           string             `json:"type"`
	FileID         pgtype.Text        `json:"file_id"`
	FilesizeBytes  int64              `json:"filesize_bytes"`
	CreatedAt      pgtype.Timestamptz `json:"created_at"`
	UpdatedAt      pgtype.Timestamptz `json:"updated_at"`
	ArchivedAt     pgtype.Timestamptz `json:"archived_at"`
	CollectionName string             `json:"collection_name"`
	CollectionID   pgtype.UUID        `json:"collection_id"`
	CollectionSlug pgtype.Text        `json:"collection_slug"`
	WorkspaceName  string             `json:"workspace_name"`
	WorkspaceID    pgtype.UUID        `json:"workspace_id"`
	WorkspaceSlug  pgtype.Text        `json:"workspace_slug"`
	ChunkID        int32              `json:"chunk_id"`
	ChunkContent   pgtype.Text        `json:"chunk_content"`
	ChunkIndex     int32              `json:"chunk_index"`
	Status         EntryStatus        `json:"status"`
	TextScore      pgtype.Float8      `json:"text_score"`
	SemanticScore  float64            `json:"semantic_score"`
	Rank           int64              `json:"rank"`
	Score          float64            `json:"score"`
}

func (q *Queries) QueryWithHybridSearch(ctx context.Context, arg QueryWithHybridSearchParams) ([]QueryWithHybridSearchRow, error) {
	rows, err := q.db.Query(ctx, queryWithHybridSearch,
		arg.Limit,
		arg.Offset,
		arg.Embedding,
		arg.WorkspacePublicID,
		arg.WorkspaceSlug,
		arg.UserID,
		arg.Query,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []QueryWithHybridSearchRow{}
	for rows.Next() {
		var i QueryWithHybridSearchRow
		if err := rows.Scan(
			&i.PublicID,
			&i.Title,
			&i.Meta,
			&i.Type,
			&i.FileID,
			&i.FilesizeBytes,
			&i.CreatedAt,
			&i.UpdatedAt,
			&i.ArchivedAt,
			&i.CollectionName,
			&i.CollectionID,
			&i.CollectionSlug,
			&i.WorkspaceName,
			&i.WorkspaceID,
			&i.WorkspaceSlug,
			&i.ChunkID,
			&i.ChunkContent,
			&i.ChunkIndex,
			&i.Status,
			&i.TextScore,
			&i.SemanticScore,
			&i.Rank,
			&i.Score,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const requeueEntries = `-- name: RequeueEntries :many
update entries_queue
set status = 'queued', attempts = 0, available_at = now()
where entry_id = any($1::integer[])
returning entry_id
`

func (q *Queries) RequeueEntries(ctx context.Context, entryIds []int32) ([]int32, error) {
	rows, err := q.db.Query(ctx, requeueEntries, entryIds)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []int32{}
	for rows.Next() {
		var entry_id int32
		if err := rows.Scan(&entry_id); err != nil {
			return nil, err
		}
		items = append(items, entry_id)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const resolveEntryIds = `-- name: ResolveEntryIds :many
select e.id, e.public_id
from entries e
where e.public_id = any($1::uuid[]) and e.deleted_at is null
`

type ResolveEntryIdsRow struct {
	ID       int32       `json:"id"`
	PublicID pgtype.UUID `json:"public_id"`
}

func (q *Queries) ResolveEntryIds(ctx context.Context, entryPublicIds []pgtype.UUID) ([]ResolveEntryIdsRow, error) {
	rows, err := q.db.Query(ctx, resolveEntryIds, entryPublicIds)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []ResolveEntryIdsRow{}
	for rows.Next() {
		var i ResolveEntryIdsRow
		if err := rows.Scan(&i.ID, &i.PublicID); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const resolveEntryIdsWithoutChunks = `-- name: ResolveEntryIdsWithoutChunks :many
select e.id, count(ec.id) as chunk_count
from entries e
left join entry_chunks ec on ec.entry_id = e.id
group by e.id
having
    e.public_id = any($1::uuid[])
    and e.deleted_at is null
    and count(ec.id) = 0
`

type ResolveEntryIdsWithoutChunksRow struct {
	ID         int32 `json:"id"`
	ChunkCount int64 `json:"chunk_count"`
}

func (q *Queries) ResolveEntryIdsWithoutChunks(ctx context.Context, entryPublicIds []pgtype.UUID) ([]ResolveEntryIdsWithoutChunksRow, error) {
	rows, err := q.db.Query(ctx, resolveEntryIdsWithoutChunks, entryPublicIds)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []ResolveEntryIdsWithoutChunksRow{}
	for rows.Next() {
		var i ResolveEntryIdsWithoutChunksRow
		if err := rows.Scan(&i.ID, &i.ChunkCount); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const updateChunk = `-- name: UpdateChunk :one
update entry_chunks
set
    content = case when $1::text is null then content else $1 end,
    language = case when $2::text is null then language else $2 end
where entry_id = $3 and chunk_index = $4 and min_version = $5
returning id, entry_id, chunk_index, min_version, content, created_at, updated_at, deleted_at, text_vector, semantic_vector, language, embedding_status, embedding_status_updated_at, last_embedding_error, last_embedding_error_at, embedding_error_count
`

type UpdateChunkParams struct {
	Content  pgtype.Text `json:"content"`
	Language pgtype.Text `json:"language"`
	EntryID  pgtype.Int4 `json:"entry_id"`
	Index    int32       `json:"index"`
	Version  int32       `json:"version"`
}

func (q *Queries) UpdateChunk(ctx context.Context, arg UpdateChunkParams) (EntryChunk, error) {
	row := q.db.QueryRow(ctx, updateChunk,
		arg.Content,
		arg.Language,
		arg.EntryID,
		arg.Index,
		arg.Version,
	)
	var i EntryChunk
	err := row.Scan(
		&i.ID,
		&i.EntryID,
		&i.ChunkIndex,
		&i.MinVersion,
		&i.Content,
		&i.CreatedAt,
		&i.UpdatedAt,
		&i.DeletedAt,
		&i.TextVector,
		&i.SemanticVector,
		&i.Language,
		&i.EmbeddingStatus,
		&i.EmbeddingStatusUpdatedAt,
		&i.LastEmbeddingError,
		&i.LastEmbeddingErrorAt,
		&i.EmbeddingErrorCount,
	)
	return i, err
}

const updateChunkEmbeddingStatus = `-- name: UpdateChunkEmbeddingStatus :exec
update entry_chunks
set
    embedding_status = $1,
    last_embedding_error = case when $2::text is null then last_embedding_error else $2 end,
    last_embedding_error_at = case when $2::text is null then last_embedding_error_at else now() end
where id = $3
`

type UpdateChunkEmbeddingStatusParams struct {
	EmbeddingStatus    EntryChunkEmbeddingStatus `json:"embedding_status"`
	LastEmbeddingError pgtype.Text               `json:"last_embedding_error"`
	ChunkID            int32                     `json:"chunk_id"`
}

func (q *Queries) UpdateChunkEmbeddingStatus(ctx context.Context, arg UpdateChunkEmbeddingStatusParams) error {
	_, err := q.db.Exec(ctx, updateChunkEmbeddingStatus, arg.EmbeddingStatus, arg.LastEmbeddingError, arg.ChunkID)
	return err
}

const updateChunkSemanticVector = `-- name: UpdateChunkSemanticVector :exec
update entry_chunks
set
    semantic_vector = $1,
    embedding_status = 'done',
    embedding_status_updated_at = now(),
    embedding_error_count = 0
where id = $2
`

type UpdateChunkSemanticVectorParams struct {
	SemanticVector pgvector.Vector `json:"semantic_vector"`
	ChunkID        int32           `json:"chunk_id"`
}

func (q *Queries) UpdateChunkSemanticVector(ctx context.Context, arg UpdateChunkSemanticVectorParams) error {
	_, err := q.db.Exec(ctx, updateChunkSemanticVector, arg.SemanticVector, arg.ChunkID)
	return err
}

const updateEntry = `-- name: UpdateEntry :one
update entries
set name = case when $1::text is null then name else $1 end,
	content = case when $2::text is null then content else $2 end,
	text_content = case when $3::text is null then text_content else $3 end,
	checksum = case when $4::text is null then checksum else $4 end
where public_id = $5 and deleted_at is null
returning id, origin, name, content, file_id, version, entry_type, checksum, parent_id, collection_id, added_by, last_updated_by, meta, created_at, updated_at, deleted_at, archived_at, filesize_bytes, public_id, text_content
`

type UpdateEntryParams struct {
	Name        pgtype.Text `json:"name"`
	Content     pgtype.Text `json:"content"`
	TextContent pgtype.Text `json:"text_content"`
	Checksum    pgtype.Text `json:"checksum"`
	PublicID    pgtype.UUID `json:"public_id"`
}

func (q *Queries) UpdateEntry(ctx context.Context, arg UpdateEntryParams) (Entry, error) {
	row := q.db.QueryRow(ctx, updateEntry,
		arg.Name,
		arg.Content,
		arg.TextContent,
		arg.Checksum,
		arg.PublicID,
	)
	var i Entry
	err := row.Scan(
		&i.ID,
		&i.Origin,
		&i.Name,
		&i.Content,
		&i.FileID,
		&i.Version,
		&i.EntryType,
		&i.Checksum,
		&i.ParentID,
		&i.CollectionID,
		&i.AddedBy,
		&i.LastUpdatedBy,
		&i.Meta,
		&i.CreatedAt,
		&i.UpdatedAt,
		&i.DeletedAt,
		&i.ArchivedAt,
		&i.FilesizeBytes,
		&i.PublicID,
		&i.TextContent,
	)
	return i, err
}

const updateEntryStatus = `-- name: UpdateEntryStatus :exec
update entries_queue
set status = $1,
    attempts = case
        when status = 'queued' then 1
        when status = 'processing' then attempts
        when status = 'failed' and attempts < max_attempts then attempts + 1
        when status = 'failed' and attempts >= max_attempts then 0
        when status = 'completed' or status = 'canceled' then attempts
        else attempts + 1
    end,
    updated_at = now()
where entry_id = $2
`

type UpdateEntryStatusParams struct {
	Status  EntryStatus `json:"status"`
	EntryID int32       `json:"entry_id"`
}

func (q *Queries) UpdateEntryStatus(ctx context.Context, arg UpdateEntryStatusParams) error {
	_, err := q.db.Exec(ctx, updateEntryStatus, arg.Status, arg.EntryID)
	return err
}
